annotated-doc==0.0.3
annotated-types==0.7.0





















































































































































































































































































































































































































































- FastAPI Performance: https://fastapi.tiangolo.com/deployment/- Prometheus Best Practices: https://prometheus.io/docs/practices/- k6 Documentation: https://k6.io/docs/- Locust Documentation: https://docs.locust.io/## References4. **Geographic distribution**: Multi-region load tests3. **Long-running tests**: 24+ hour soak tests2. **Network partitioning**: Test split-brain scenarios1. **Chaos engineering**: Kill random services### Advanced Testing4. **Connection pooling**: Tune pool sizes for load3. **Async all the way**: Ensure no blocking I/O2. **Caching layer**: Redis cache for policies/configs1. **Batch processing**: Group database operations### Optimization4. **Queue partitioning**: Use multiple Redis instances3. **Database sharding**: Partition data across databases2. **Worker scaling**: Run multiple worker processes1. **Horizontal scaling**: Deploy multiple API instances behind load balancer### Scaling## Next Steps4. **HTTP/2**: Enable HTTP/2 in uvicorn3. **Reduce logging**: Lower log level in production2. **Database optimization**: Connection pooling, prepared statements1. **Enable async processing**: Use queue for slow operations### Reduce Latency4. **Cache frequently accessed data**: Redis caching3. **Optimize queries**: Add indexes, use batch operations2. **Tune database**: Increase connection pool size1. **Scale workers**: Run multiple worker instances### Increase Throughput## Performance Tuning- Worker throughput metrics- Database write performance- Remediator service availability- Worker service statusCheck:### Queue not draining- Worker service running- Redis connection availability- Database connection pool size- Rate limiting configurationCheck:### High failure rate- Dependencies installed (`pip install httpx`)- Port 8000 is accessible- API server is runningCheck:### Simulator won't start## Troubleshooting```pytest python/tests/ -v --cov=app --cov-report=html# Full test suitepytest python/tests/test_simulator.py::TestSimulatorIntegration -v# Integration tests  pytest python/tests/test_simulator.py -v# Unit tests```bashRun comprehensive integration tests:## Integration Tests- [ ] No memory leaks during extended tests- [ ] Audit trail remains intact under load- [ ] System recovers gracefully after overload- [ ] Retry logic handles transient failures- [ ] Rate limiting activates at configured thresholds- [ ] Worker processes all queued tasks- [ ] Queue drains after burst traffic- [ ] Error rate < 5% during normal operation- [ ] P95 latency stays below 1 second under load- [ ] API responds to all requests (no dropped connections)### Validation Checklist   - Redis connections   - Database connections   - Memory usage   - CPU usage5. **System Resources**   - Retry-After times   - Rate limit hits by endpoint   - 429 response count4. **Rate Limiting**   - Average processing time   - Success rate   - Tasks processed/minute   - Worker active status3. **Worker Metrics**   - Tasks failed   - Tasks completed   - Dequeue rate   - Enqueue rate   - Queue length (current depth)2. **Queue Metrics**   - P50, P95, P99 latency   - Error rate (4xx, 5xx)   - Success rate   - Total requests/second1. **Request Metrics**### During Load Tests## Metrics to Monitor- System recovers after load reduces- No crashes or data loss- Response times degrade gracefully- Queue depth increases- Rate limiting engages**Expected:**```curl -X POST "http://localhost:8000/api/v1/ui/simulator/start?rate=10000&mode=steady"# Or extreme simulator ratek6 run --env SCENARIO=ramp tests/load_test.k6.js# Ramp to high load```bashTest system limits:### Scenario 4: Overload Test- System recovers after failures- Timeouts handled gracefully- Malformed requests rejected (400)- Retry logic activates**Expected:**```  --users 20 --spawn-rate 5 --run-time 5m --headless --tags failurelocust -f tests/locustfile.py --host=http://localhost:8000 \# Locust failure injectorcurl -X POST "http://localhost:8000/api/v1/ui/simulator/start?rate=500&mode=chaos&failure_rate=0.1&timeout_rate=0.05&malformed_rate=0.02"# Simulator with failures```bashTest error handling:### Scenario 3: Failure Injection- Rate limiting activates above threshold- Worker processes backlog- Queue depth increases during spikes- System handles bursts gracefully**Expected:**```curl -X POST "http://localhost:8000/api/v1/ui/simulator/start?rate=2000&mode=burst"# Simulator  --users 200 --spawn-rate 50 --run-time 5m --headless --tags burstlocust -f tests/locustfile.py --host=http://localhost:8000 \# Locust```bashTest spike handling:### Scenario 2: Burst Traffic- No rate limiting- Queue depth stable- Error rate < 1%- P95 latency < 500ms**Expected:**```curl -X POST "http://localhost:8000/api/v1/ui/simulator/start?rate=1000&mode=steady"# Simulatork6 run --vus 50 --duration 10m tests/load_test.k6.js# k6  --users 50 --spawn-rate 5 --run-time 10m --headless --tags steadylocust -f tests/locustfile.py --host=http://localhost:8000 \# Locust```bashTest sustained throughput:### Scenario 1: Steady Load Test## Testing Scenarios```}  "events_succeeded": 2327  "events_generated": 2450,  "runtime_seconds": 296,  "event": "simulator_stopped",{}  "metric_name": "cpu_usage"  "event": "simulator_timeout",{}  "status": 429  "event": "simulator_rate_limited",{}  "mode": "burst"  "rate": 500,  "event": "simulator_started",{```jsonStructured logging for simulator events:#### Logs```histogram_quantile(0.95, request_latency_seconds_bucket)# API latencyrate(worker_tasks_total[1m])# Worker throughputqueue_length# Queue depthrate(actions_total[1m])# Action triggersrate(ingest_total[1m])# Ingestion rate```promqlSimulator activity is automatically tracked by existing metrics:#### Prometheus Metrics### 5. Observability- Success rate percentage- Actual vs target rate- Malformed payload count- Timeout count- Rate limiting hits- Events generated/succeeded/failed**Simulator Metrics:**- Runtime tracking- Rate limiting detection- Success/failure counters- Real-time metrics display- Start/stop buttons with configuration**Features:**The web dashboard ([http://localhost:8000/ui/dashboard](http://localhost:8000/ui/dashboard)) includes simulator controls:### 4. Dashboard Integration```}  }    }      "actual_rate": 496.5      "events_malformed": 12,      "events_timeout": 23,      "events_rate_limited": 45,      "events_failed": 123,      "events_succeeded": 2327,      "events_generated": 2450,    "metrics": {    },      "failure_rate": 0.05      "mode": "burst",      "target_rate": 500,    "configuration": {    "runtime_seconds": 296,    "started_at": "2026-01-19T12:30:00",    "running": true,  "simulator": {  "timestamp": "2026-01-19T12:34:56",  "ok": true,{```json**Response:**```GET /api/v1/ui/simulator/status```http#### Get Status```POST /api/v1/ui/simulator/stop```http#### Stop Simulator```  &malformed_rate=0.01  &timeout_rate=0.02  &failure_rate=0.05  &mode=burst  ?rate=500POST /api/v1/ui/simulator/start```http#### Start Simulator### 3. API Endpoints- `ramp`: Gradual ramp 0→50→100→150→0 over 8 minutes- `burst`: Ramp 0→100→0→200→0 over 4 minutes- `steady`: Constant 50 VUs for 5 minutes**Scenarios:**```k6 run --out json=results.json tests/load_test.k6.js# With outputk6 run --env SCENARIO=burst tests/load_test.k6.js# Burst scenariok6 run --vus 50 --duration 5m tests/load_test.k6.js# Custom settingsk6 run tests/load_test.k6.js# Basic test```bash**Run k6:**JavaScript-based load testing with detailed metrics:#### k6 (`tests/load_test.k6.js`)- `FailureInjector`: Malformed requests and errors- `BurstUser`: Periodic burst traffic- `VigilUser`: Realistic mixed workload (ingest + queries)**User Types:**```locust -f tests/locustfile.py --host=http://localhost:8000 --tags steady# Specific scenario  --headless  --run-time 5m \  --spawn-rate 10 \  --users 100 \  --host=http://localhost:8000 \locust -f tests/locustfile.py \# Headless modelocust -f tests/locustfile.py --host=http://localhost:8000# With web UI```bash**Run Locust:**Python-based load testing with web UI:#### Locust (`tests/locustfile.py`)### 2. Load Testing Scripts```await simulator.stop()# Stopprint(f"Succeeded: {status['metrics']['events_succeeded']}")print(f"Generated: {status['metrics']['events_generated']}")status = simulator.get_status()# Get statusawait simulator.start()# Start)    malformed_rate=0.01          # 1% malformed payloads    timeout_rate=0.02,           # 2% timeout rate    failure_rate=0.05,           # 5% failure rate    mode=SimulatorMode.BURST,    # Burst traffic pattern    rate=500,                    # 500 events per minutesimulator.configure(# Configuresimulator = get_simulator()# Get simulator instancefrom app.services.simulator import get_simulator, SimulatorMode```python**Usage:**- API control endpoints- Real-time metrics tracking- Failure injection: Timeouts, malformed payloads, HTTP errors- Operating modes: Steady, Burst, Ramp, Chaos- Configurable event rates (1-10,000 events/min)**Features:**Asynchronous event generator with multiple operating modes:### 1. Simulator Service (`app/services/simulator.py`)## Components- **Recovery Testing**: Validates system resilience under stress- **Observability**: Real-time metrics and dashboard integration- **Failure Injection**: Tests retry logic, rate limiting, and error handling- **Load Testing**: Locust and k6 scripts for realistic load patterns  - **Simulator Service**: Generates synthetic events at configurable ratesPhase 5 implements comprehensive load testing and simulation capabilities for the Vigil monitoring system:## Overviewanyio==4.11.0
click==8.3.0
colorama==0.4.6
fastapi==0.120.0
h11==0.16.0
httpx==0.24.1
idna==3.11
pydantic==2.12.3
pydantic_core==2.41.4
sniffio==1.3.1
starlette==0.48.0
typing-inspection==0.4.2
typing_extensions==4.15.0
uvicorn==0.38.0
prometheus-client==0.21.0
psycopg2-binary==2.9.9
asyncpg==0.29.0
redis==5.0.0
requests==2.31.0
pyyaml==6.0.1
python-dotenv==1.0.0
